Index: models/gat_layer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass GATLayer(nn.Module):\r\n    # TODO could also create a MessagePassing class first and use that, similar to how torch_geometric's GATConv class works\r\n    # i.e. class MessagePassing(torch.nn.Module): ...\r\n    \"\"\"\r\n    GAT layer implementation\r\n    \"\"\"\r\n\r\n    # Extra possible params from torch_geometric: (add_self_loops: bool = True, bias: bool = True, **kwargs for MessagePassing)\r\n    def __init__(self,in_channels, out_channels, dropout, alpha, concat = True):\r\n        super(GATLayer, self).__init__()\r\n\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n        self.dropout = dropout # should be 0.6\r\n        self.alpha = alpha # should be 0.2\r\n        self.concat = concat\r\n\r\n        # model should be initialised using Glorot(Xavier initialisation) which means:\r\n        # leakyReLu with negative slope = 0.2\r\n        # there is also an in-built initialisation in pytorch for Xavier initialisation\r\n\r\n        self.weights = nn.Parameter(torch.zeros(size=(in_channels, out_channels)))\r\n        gain = nn.init.calculate_gain('relu') # this should return sqrt(2), which is the recommended value for the\r\n                                              # non-linearity function relu\r\n\r\n        nn.init.xavier_uniform_(self.weights.data, gain=gain)\r\n        self.attention = nn.Parameter(torch.zeros(size=(2*out_channels,1))) # 2 because we will concatenate the two\r\n                                                                            # representations and we need to match the\r\n                                                                            # dimension\r\n\r\n        nn.init.xavier_uniform_(self.attention.data, gain=gain)\r\n        self.leakyReLu = nn.LeakyReLU(self.alpha)\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        \"\"\"Reinitialize learnable parameters.\"\"\"\r\n        # not very sure what to put here maybe the xavier weights?\r\n\r\n    def forward(self, x, immediate_neighbor):\r\n        h = torch.mm(x, self.weights) # the initial linear transformation\r\n        n = h.size()[0] # extracting the size of h (F in the paper)\r\n\r\n        # attention\r\n        input_attention = torch.cat([h.repeat(1, n).view(n*n, -1), h.repeat(n, 1)], dim=1).view(n, -1,\r\n                                                                                                2*self.out_channels)\r\n        attention_coefficients = self.leakyReLu(torch.matmul(input_attention, self.attention).squeeze(right_dimension))\r\n\r\n                                                        # squeeze output in the right_dimension - not sure if it should\r\n                                                        # be dimension 1 or 2, need to look at some data\r\n        # masked attention\r\n\r\n        #negative_vector = -10e20*torch.ones_like(attention_coefficients)\r\n        masked_attention = torch.where(immediate_neighbor > 0, attention_coefficients, 0.)\r\n\r\n                                         # we have attention only for immediate neighbors, otherwise we replace them\r\n                                         # with 0. NOTE: instead of 0 maybe we can put something really negative,\r\n                                         # like negative_vector\r\n        # for immediate neighbors\r\n        # otherwise we replace with 0\r\n        # here instead of 0 maybe\r\n\r\n        masked_attention = F.softmax(masked_attention, dim=1)\r\n        masked_attention = F.dropout(masked_attention, self.dropout, training=self.training)\r\n        h_hat = torch.matmul(masked_attention, h)\r\n\r\n        # should return the weights in the forward to help with the visualisation\r\n        weights = self.weights\r\n\r\n        if self.concat:\r\n            return F.elu(h_hat)\r\n        else:\r\n            return h_hat\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models/gat_layer.py b/models/gat_layer.py
--- a/models/gat_layer.py	(revision c229d4860c715745a9007e89899ae86703d4b9c9)
+++ b/models/gat_layer.py	(date 1615849224463)
@@ -12,29 +12,35 @@
     """
 
     # Extra possible params from torch_geometric: (add_self_loops: bool = True, bias: bool = True, **kwargs for MessagePassing)
-    def __init__(self,in_channels, out_channels, dropout, alpha, concat = True):
+    def __init__(self, in_channels, out_channels, dropout, alpha, number_of_heads, concat=True, include_skip_connection = False):
         super(GATLayer, self).__init__()
 
         self.in_channels = in_channels
         self.out_channels = out_channels
-        self.dropout = dropout # should be 0.6
-        self.alpha = alpha # should be 0.2
+        self.dropout = dropout  # should be 0.6
+        self.alpha = alpha  # should be 0.2
+        self.number_of_heads = number_of_heads
         self.concat = concat
+        self.include_skip_connection = include_skip_connection # useful for the transductive implementation
 
         # model should be initialised using Glorot(Xavier initialisation) which means:
         # leakyReLu with negative slope = 0.2
         # there is also an in-built initialisation in pytorch for Xavier initialisation
 
         self.weights = nn.Parameter(torch.zeros(size=(in_channels, out_channels)))
-        gain = nn.init.calculate_gain('relu') # this should return sqrt(2), which is the recommended value for the
-                                              # non-linearity function relu
+        gain = nn.init.calculate_gain('relu')  # this should return sqrt(2), which is the recommended value for the
+                                               # non-linearity function relu
 
         nn.init.xavier_uniform_(self.weights.data, gain=gain)
-        self.attention = nn.Parameter(torch.zeros(size=(2*out_channels,1))) # 2 because we will concatenate the two
-                                                                            # representations and we need to match the
-                                                                            # dimension
+        self.attention = nn.Parameter(torch.zeros(size=(2 * out_channels, 1))) # 2 because we will concatenate the two
+                                                                               # representations and we need to match the
+                                                                               # dimension
 
         nn.init.xavier_uniform_(self.attention.data, gain=gain)
+        1
+        if include_skip_connection_skip_connection:
+            self.skip_projections = nn.Linear(in_channels, no_of_heads * out_channels, bias=False)
+
         self.leakyReLu = nn.LeakyReLU(self.alpha)
         self.reset_parameters()
 
@@ -43,27 +49,20 @@
         # not very sure what to put here maybe the xavier weights?
 
     def forward(self, x, immediate_neighbor):
-        h = torch.mm(x, self.weights) # the initial linear transformation
-        n = h.size()[0] # extracting the size of h (F in the paper)
+        h = torch.mm(x, self.weights)  # the initial linear transformation
+        n = h.size()[0]  # extracting the size of h (F in the paper)
 
         # attention
-        input_attention = torch.cat([h.repeat(1, n).view(n*n, -1), h.repeat(n, 1)], dim=1).view(n, -1,
-                                                                                                2*self.out_channels)
-        attention_coefficients = self.leakyReLu(torch.matmul(input_attention, self.attention).squeeze(right_dimension))
+        input_attention = torch.cat([h.repeat(1, n).view(n * n, -1), h.repeat(n, 1)], dim=1).view(n, -1, 2 * self.out_channels)
 
-                                                        # squeeze output in the right_dimension - not sure if it should
-                                                        # be dimension 1 or 2, need to look at some data
+        attention_coefficients = self.leakyReLu(torch.matmul(input_attention, self.attention))
+        attention_coefficients = attention_coefficients.squeeze(2)
+
         # masked attention
-
-        #negative_vector = -10e20*torch.ones_like(attention_coefficients)
-        masked_attention = torch.where(immediate_neighbor > 0, attention_coefficients, 0.)
-
-                                         # we have attention only for immediate neighbors, otherwise we replace them
-                                         # with 0. NOTE: instead of 0 maybe we can put something really negative,
-                                         # like negative_vector
-        # for immediate neighbors
-        # otherwise we replace with 0
-        # here instead of 0 maybe
+        negative_vector = -10e20*torch.ones_like(attention_coefficients)
+        masked_attention = torch.where(immediate_neighbor > 0, attention_coefficients, negative_vector) # to avoid any
+                                                                # computation with 0, as that might create problems with
+                                                                # backpropagation and attention updates
 
         masked_attention = F.softmax(masked_attention, dim=1)
         masked_attention = F.dropout(masked_attention, self.dropout, training=self.training)
@@ -77,3 +76,16 @@
         else:
             return h_hat
 
+    def skip_connections(self, input_node_features, output_node_features):
+
+        if self.include_skip_connection:
+            if output_node_features.shape[-1] == input_node_features.shape[-1]:
+                output_node_features += input_node_features.unsqueeze(1)
+            else:
+                output_node_features += self.skip_projection(input_nodes_features).view(-1, self.number_of_heads, self.out_channels)
+
+        if self.concat:
+            output_node_features = output_node_features.view(-1, self.number_of_heads * self.out_channels)
+        else:
+            output_node_features = output_node_features.mean(dim=1)
+
Index: models/gat_layer_2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch.nn as nn\r\n\r\nclass GATLayer_2(nn.Module):\r\n\r\n    def __init__(self, input_features, output_features, no_heads, dropout=0.6, concat=True):\r\n\r\n        super(GATLayer_2, self).__init__()\r\n\r\n        self.no_heads = no_heads\r\n        self.input_features = input_features\r\n        self.output_features = output_features\r\n        self.ls = nn.Linear(input_features, no_heads*output_features, bias=False) # ls stands for linear sum which is\r\n                                                                                  # the first linear projection defined\r\n                                                                                  # in the paper\r\n        # dimension here is 1 to denote that we are calculating the attention of 1 node\r\n        # second dimension denotes the number of heads, which have independent feature representation\r\n        # third dimension is just the number of output features from previous layers/representations\r\n\r\n        self.self_attention_coefficient = nn.Parameter(torch.Tensor(1,no_heads, output_features)) #self-attention needs\r\n                                                                                    # to be defined even if there is no\r\n                                                                                    # self-loop\r\n        self.neighbor_attention_coefficient = nn.Parameter(torch.Tensor(1,no_heads, output_features))\r\n\r\n\r\n        #could have activation as an input to the class\r\n        #activation = nn.ELU()\r\n        #self.activation = activation\r\n        self.leakyReLu = nn.LeakyRelu(0.2) # negative slope is 0.2 from the paper\r\n        self.dropout = nn.Dropout(dropout)\r\n        self.attention_weights = None\r\n        self.concat = concat\r\n        self.reset_parameters()\r\n\r\n    def forward(self, input):\r\n        node_features, edge_index = input\r\n        total_nodes = node_features.shape[0] # first dimension gives the total number of nodes from the input features\r\n        node_features = self.dropout(node_features) # we apply a dropout on all input features as described in the paper\r\n        node_features_projection = self.\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models/gat_layer_2.py b/models/gat_layer_2.py
--- a/models/gat_layer_2.py	(revision c229d4860c715745a9007e89899ae86703d4b9c9)
+++ b/models/gat_layer_2.py	(date 1615843614534)
@@ -12,6 +12,7 @@
         self.ls = nn.Linear(input_features, no_heads*output_features, bias=False) # ls stands for linear sum which is
                                                                                   # the first linear projection defined
                                                                                   # in the paper
+
         # dimension here is 1 to denote that we are calculating the attention of 1 node
         # second dimension denotes the number of heads, which have independent feature representation
         # third dimension is just the number of output features from previous layers/representations
@@ -31,9 +32,19 @@
         self.concat = concat
         self.reset_parameters()
 
-    def forward(self, input):
-        node_features, edge_index = input
+    def forward(self, input_data):
+        node_features, edge_index = input_data
         total_nodes = node_features.shape[0] # first dimension gives the total number of nodes from the input features
         node_features = self.dropout(node_features) # we apply a dropout on all input features as described in the paper
-        node_features_projection = self.
+
+        # we projecting the feature representations of the nodes
+        projection_node_features = self.ls(node_features)
+        projection_node_features = projection_node_features.view(-1, self.no_heads, self.output_features) # reshape
+        projection_node_features = self.dropout(projection_node_features) # in the paper they drop features
+
+        # calculate attention scores between edges
+        self_node_attention = torch.sum(projection_node_features*self.self_attention_coefficient, dim=1)
+        neighbor_node_attention = torch.sump(rojection_node_features*self.neighbor_attention_coefficient, dim=1)
+
+        # now we calculate the attention for the nodes that have common edges (i.e. are connected)
 
