Index: models/gat_layer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass GATLayer(nn.Module):\r\n    # TODO could also create a MessagePassing class first and use that, similar to how torch_geometric's GATConv class works\r\n    # i.e. class MessagePassing(torch.nn.Module): ...\r\n    \"\"\"\r\n    GAT layer implementation\r\n    \"\"\"\r\n\r\n    # Extra possible params from torch_geometric: (add_self_loops: bool = True, bias: bool = True, **kwargs for MessagePassing)\r\n    def __init__(self,in_channels, out_channels, dropout, alpha, concat = True):\r\n        super(GATLayer, self).__init__()\r\n\r\n        self.in_channels = in_channels\r\n        self.out_channels = out_channels\r\n        self.dropout = dropout # should be 0.6\r\n        self.alpha = alpha # should be 0.2\r\n        self.concat = concat\r\n\r\n        # model should be initialised using Glorot(Xavier initialisation) which means:\r\n        # leakyReLu with negative slope = 0.2\r\n        # there is also an in-built initialisation in pytorch for Xavier initialisation\r\n\r\n        self.weights = nn.Parameter(torch.zeros(size=(in_channels, out_channels)))\r\n        gain = nn.init.calculate_gain('relu') # this should return sqrt(2), which is the recommended value for the\r\n                                              # non-linearity function relu\r\n\r\n        nn.init.xavier_uniform_(self.weights.data, gain=gain)\r\n        self.attention = nn.Parameter(torch.zeros(size=(2*out_channels,1))) # 2 because we will concatenate the two\r\n                                                                            # representations and we need to match the\r\n                                                                            # dimension\r\n\r\n        nn.init.xavier_uniform_(self.attention.data, gain=gain)\r\n        self.leakyReLu = nn.LeakyReLU(self.alpha)\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        \"\"\"Reinitialize learnable parameters.\"\"\"\r\n        # not very sure what to put here maybe the xavier weights?\r\n\r\n    def forward(self, x, immediate_neighbor):\r\n        h = torch.mm(x, self.weights) # the initial linear transformation\r\n        n = h.size()[0] # extracting the size of h (F in the paper)\r\n\r\n        # attention\r\n        input_attention = torch.cat([h.repeat(1, n).view(n*n, -1), h.repeat(n, 1)], dim=1).view(n, -1,\r\n                                                                                                2*self.out_channels)\r\n        attention_coefficients = self.leakyReLu(torch.matmul(input_attention, self.attention).squeeze(right_dimension))\r\n\r\n                                                        # squeeze output in the right_dimension - not sure if it should\r\n                                                        # be dimension 1 or 2, need to look at some data\r\n        # masked attention\r\n\r\n        #negative_vector = -10e20*torch.ones_like(attention_coefficients)\r\n        masked_attention = torch.where(immediate_neighbor > 0, attention_coefficients, 0.)\r\n\r\n                                         # we have attention only for immediate neighbors, otherwise we replace them\r\n                                         # with 0. NOTE: instead of 0 maybe we can put something really negative,\r\n                                         # like negative_vector\r\n        # for immediate neighbors\r\n        # otherwise we replace with 0\r\n        # here instead of 0 maybe\r\n\r\n        masked_attention = F.softmax(masked_attention, dim=1)\r\n        masked_attention = F.dropout(masked_attention, self.dropout, training=self.training)\r\n        h_hat = torch.matmul(masked_attention, h)\r\n\r\n        # should return the weights in the forward to help with the visualisation\r\n        weights = self.weights\r\n\r\n        if self.concat:\r\n            return F.elu(h_hat)\r\n        else:\r\n            return h_hat\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models/gat_layer.py b/models/gat_layer.py
--- a/models/gat_layer.py	(revision f2de6569806d8ad4ed072d51b00cedf88a2a5877)
+++ b/models/gat_layer.py	(date 1615834115711)
@@ -76,4 +76,3 @@
             return F.elu(h_hat)
         else:
             return h_hat
-
Index: models/gat_layer_2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models/gat_layer_2.py b/models/gat_layer_2.py
new file mode 100644
--- /dev/null	(date 1615834245123)
+++ b/models/gat_layer_2.py	(date 1615834245123)
@@ -0,0 +1,39 @@
+import torch.nn as nn
+
+class GATLayer_2(nn.Module):
+
+    def __init__(self, input_features, output_features, no_heads, dropout=0.6, concat=True):
+
+        super(GATLayer_2, self).__init__()
+
+        self.no_heads = no_heads
+        self.input_features = input_features
+        self.output_features = output_features
+        self.ls = nn.Linear(input_features, no_heads*output_features, bias=False) # ls stands for linear sum which is
+                                                                                  # the first linear projection defined
+                                                                                  # in the paper
+        # dimension here is 1 to denote that we are calculating the attention of 1 node
+        # second dimension denotes the number of heads, which have independent feature representation
+        # third dimension is just the number of output features from previous layers/representations
+
+        self.self_attention_coefficient = nn.Parameter(torch.Tensor(1,no_heads, output_features)) #self-attention needs
+                                                                                    # to be defined even if there is no
+                                                                                    # self-loop
+        self.neighbor_attention_coefficient = nn.Parameter(torch.Tensor(1,no_heads, output_features))
+
+
+        #could have activation as an input to the class
+        #activation = nn.ELU()
+        #self.activation = activation
+        self.leakyReLu = nn.LeakyRelu(0.2) # negative slope is 0.2 from the paper
+        self.dropout = nn.Dropout(dropout)
+        self.attention_weights = None
+        self.concat = concat
+        self.reset_parameters()
+
+    def forward(self, input):
+        node_features, edge_index = input
+        total_nodes = node_features.shape[0] # first dimension gives the total number of nodes from the input features
+        node_features = self.dropout(node_features) # we apply a dropout on all input features as described in the paper
+        node_features_projection = self.
+
